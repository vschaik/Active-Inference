{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Free Energy Principle.\n",
    "\n",
    "## By André van Schaik\n",
    "\n",
    "### __[International Centre for Neuromorphic Systems](https://westernsydney.edu.au/icns)__        25/12/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some derivations in regards to Karl Friston's Free Energy Principle. It's a bit of a scratch pad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimally estimate the hidden state of its environment, $\\theta$, from its noisy sensory input, $\\phi$, an agent would like to calculate the Bayesian posterior probability density $p(\\theta|\\phi)$. However, in all but the simplest cases, this might not be directly calculable. *Variational Bayes* suggests a workaround by minimising the Kullback-Leibler divergence between what it believes the state of its environment is (encoded in a Recognition density $q(\\theta)$) and the true Bayesian posterior.\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(\\: q(\\theta) \\; || \\; p(\\theta|\\phi) \\: ) = \\int{q(\\theta) \\: ln \\frac{q(\\theta)}{p(\\theta|\\phi)} \\: d\\theta}\n",
    "\\end{align*}\n",
    "\n",
    "The KL divergence is a measure of the difference between two probability distributions, is always positve, and is 0 if and only if the two distributions are the same. Thus adapting $q(\\theta)$ to minimise this KL divergence will result in $q(\\theta)$ being a close approximation of $p(\\theta|\\phi)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, to evaluate (1) directly, we would still need to be able to calculate $p(\\theta|\\phi)$ and we seem to have made no progress. However, the FEP uses the fact that $p(\\theta,\\phi) = p(\\theta|\\phi)p(\\phi)$, to write this as:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(\\: q(\\theta) \\; || \\; p(\\theta|\\phi) \\: ) &= \\int{q(\\theta) \\: ln \\frac{q(\\theta)}{p(\\theta,\\phi)/p(\\phi)} \\: d\\theta} \\\\\n",
    "&= \\int{q(\\theta) \\: \\{ ln\\:q(\\theta) - ln\\:p(\\theta,\\phi) + ln\\:p(\\phi) \\} \\: d\\theta} \\\\\n",
    "&= \\int{q(\\theta) \\: ln \\frac{q(\\theta)}{p(\\theta,\\phi)} \\: d\\theta} + \\int{q(\\theta) \\: ln\\:p(\\phi) \\: d\\theta} \\\\\n",
    "&= \\int{q(\\theta) \\: ln \\frac{q(\\theta)}{p(\\theta,\\phi)} \\: d\\theta} + ln\\:p(\\phi) \\int{q(\\theta) \\: d\\theta} \\\\\n",
    "&= \\int{q(\\theta) \\: ln \\frac{q(\\theta)}{p(\\theta,\\phi)} \\: d\\theta} + ln\\:p(\\phi) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "since $\\int{q(\\theta) \\: d\\theta} = 1$ by definition of a probability density. We continue by writing:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(\\: q(\\theta) \\; || \\; p(\\theta|\\phi) \\: ) = F + ln\\:p(\\phi)\\\\\n",
    "F = \\int{q(\\theta) \\: ln \\frac{q(\\theta)}{p(\\theta,\\phi)} \\: d\\theta} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The joint density $p(\\theta,\\phi)$ is called the generative density, and represents the agent's belief in how the world works. It can be factorised into $p(\\theta,\\phi) = p(\\phi,\\theta) = p(\\phi|\\theta)\\:p(\\theta)$ where a prior $p(\\theta)$ encodes the agent's beliefs for the world states prior to new sensory input, and a likelihood $p(\\phi|\\theta)$ encodes how the agent's sensory signals relate to the world states. Thus, if we have a model for how the world states generate sensory perception (or if we can learn one), we can calculate $F$, which is called the *Variational Free Energy*, and is the KL divergence between the Recognition density, $q(\\theta)$, and the Generative density, $p(\\theta, \\phi)$. We probably don't know $p(\\phi)$, but, since this doesn't depend on $\\theta$, it plays no role in optimising $q(\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "F &= \\int{q(\\theta) \\: ln \\frac{q(\\theta)}{p(\\theta,\\phi)} \\: d\\theta} \\\\\n",
    "&= \\int{q(\\theta) \\: ln \\: q(\\theta) \\: d\\theta} - \\int{q(\\theta) \\: ln \\: p(\\theta,\\phi) \\: d\\theta} \\\\\n",
    "&= \\int{q(\\theta) \\: ln \\: q(\\theta) \\: d\\theta} + \\int{q(\\theta) \\: E(\\theta,\\phi) \\: d\\theta} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{align*}\n",
    "E = -ln \\: p(\\theta,\\phi)\n",
    "\\end{align*}\n",
    "\n",
    "The first term in $F$ is just the definition of negative entropy of $q$. The second is the average of $E$ over $q$, which is called *average energy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Dempster A.P., Laird, N.M. and Rubin, D.B. (1977). [Maximum likelihood from incomplete data via the EM algorithm](https://www.jstor.org/stable/2984875). Journal of the Royal Statistical Society. Series B (Methodological), 39.1 ,1-38.\n",
    "\n",
    "Neal R.M., Hinton G.E. (1998) [A view of the EM algorithm that justifies incremental, sparse, and other variants](http://www.cs.toronto.edu/~fritz/absps/emk.pdf). In: Jordan M.I. (eds) Learning in Graphical Models. NATO ASI Series (Series D: Behavioural and Social Sciences), vol 89. Springer, Dordrecht\n",
    "\n",
    "Rao, R.P., and Ballard, D.H. (1999). [Predictive coding in the visual cortex: a functional\n",
    "interpretation of some extra-classical receptive-field effects](https://www.researchgate.net/publication/13103385_Predictive_Coding_in_the_Visual_Cortex_a_Functional_Interpretation_of_Some_Extra-classical_Receptive-field_Effects). Nature Neuroscience,\n",
    "2, 79–87.\n",
    "\n",
    "Buckley C.L., Kim, C.S., McGregor, S. and Seth, A.K. (2017) [The free energy principle for action and perception: a mathematical review](https://www.sciencedirect.com/science/article/pii/S0022249617300962). Journal of Mathematical Psychology, 81, 55-79.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
