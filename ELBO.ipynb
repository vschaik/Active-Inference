{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference in 5 minutes #\n",
    "\n",
    "(copied with a few changes from [2 posts](http://davmre.github.io/blog/inference/2015/11/13/elbo-in-5min) by Dave Moore)\n",
    "\n",
    "Let $p(x,z)$ be a probability model with observed variables $x$ and latent variables $z$. We want to infer the posterior $p(z|x)$, but in general this won’t have any nice form that we can write down. Instead, we’ll pick some approximating family $q(z;\\lambda)$, with parameters $\\lambda$, and then try to find the distribution within this family that best approximates the posterior. For example, if we model each latent variable independently (a “mean field” approximation) using a scalar Gaussian, the parameters $\\lambda$ are just the means and standard deviations of these Gaussians.\n",
    "\n",
    "A natural approach to fitting the approximation parameters $\\lambda$ is to minimize the [KL divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) between our approximation $q(z;\\lambda)$ and the posterior $p(z|x)$.$^2$ Writing this out,\n",
    "\n",
    "\\begin{equation}\n",
    "KL[q(z ; \\lambda) \\| p(z | x)]=\\int q(z ; \\lambda) \\log \\frac{q(z ; \\lambda)}{p(z | x)} d z ,\n",
    "\\end{equation}\n",
    " \n",
    "we see that it depends on the posterior density $p(z|x)$ which we don’t know. However, we do have access to the joint distribution $p(x,z)$, which is proportional to the posterior, so we can just apply simple algebra to unpack the normalizing constant:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "KL[q(z ; \\lambda) \\| p(z | x)] &=\\int q(z ; \\lambda) \\log \\frac{q(z ; \\lambda)}{p(z | x)} d z \\\\\n",
    "&=\\int q(z ; \\lambda)[\\log q(z ; \\lambda)-\\log p(z | x)] d z \\\\\n",
    "&=\\int q(z ; \\lambda)\\left[\\log q(z ; \\lambda)-\\log \\frac{p(x, z)}{p(x)}\\right] d z \\\\\n",
    "&=\\log p(x)+\\int q(z ; \\lambda)[\\log q(z ; \\lambda)-\\log p(x, z)] d z \\\\\n",
    "&=\\log p(x)-\\mathcal{F}(\\lambda ; x)\n",
    "\\end{aligned}\n",
    "\\end{equation} \n",
    "\n",
    "This shows that the KL divergence is equal to the model evidence $\\log{p(x)}$, which is an (unknown) normalizing constant, minus a term $\\mathcal{F}$ given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{F}(\\lambda ; x)=\\int q(z ; \\lambda)[\\log p(x, z)-\\log q(z ; \\lambda)] d z\n",
    "\\end{equation}\n",
    "\n",
    "This term is alternately referred to as (negative) variational free energy or the evidence lower bound (ELBO). It is a lower bound on $\\log{p(x)}$\n",
    " because we can write $\\log{p(x)}=\\mathcal{F} + KL[q(z;λ)‖p(z|x)]$\n",
    " and the KL divergence is nonnegative. Since the model evidence is constant, maximizing $\\mathcal{F}$\n",
    " minimizes the KL divergence.\n",
    "\n",
    "This is the core of variational inference: pick an approximating family and minimize KL divergence between your approximation and the true posterior. \n",
    "\n",
    "The practical difficulty tends to be that $\\mathcal{F}$ involves an expectation, so evaluating and optimizing it requires either model-specific math$^2$ or Monte Carlo techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach is to note that $\\mathcal{F}$ is really just an expectation with respect to our approximating distribution $q$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{F}(\\lambda ; x) &=E_{z \\sim q}[\\log p(x, z)-\\log q(z ; \\lambda)] \\\\\n",
    "&=E_{z \\sim q}[\\log p(x, z)]+H(q ; \\lambda)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where we’ve made the simplifying assumption that the entropy $H(q;\\lambda)$ is available in closed form. This is true for Gaussian approximating families, but if we’re using some other weird family we can always move the entropy back into the Monte Carlo approximation. The expectation over $\\log{p(x,z)}$ might not have a closed form, but we can approximate it by drawing $n$ samples $z_i ∼ q(z;\\lambda)$ and evaluating the empirical expectation\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathcal{F}}(\\lambda ; x)=\\frac{1}{n} \\sum_{i=1}^{n} \\log p\\left(x, z_{i}\\right)+H(q ; \\lambda)\n",
    "\\end{equation}\n",
    "\n",
    "Our approach will be to do gradient ascent on this Monte Carlo approximation. But wait, you might object, $\\lambda$ doesn’t appear anywhere in (the Monte Carlo part of) this expression, so how can we compute a gradient? The answer is that $\\lambda$ was a parameter of the distribution that produced $z$, so we just have to differentiate through the sampling algorithm, holding fixed the random seed (this is the “reparameterization trick” $^3$ ). In many cases this is straightforward to do.\n",
    "\n",
    "For example, if $q$ is Gaussian parameterized by a mean and standard deviation $\\lambda=(\\mu,\\sigma)$, a typical sampling procedure would first sample a standard Gaussian variable $\\varepsilon \\sim N(0,1)$\n",
    " and then compute the transform $z = \\sigma \\varepsilon + \\mu$. Rewriting our Monte Carlo ELBO in terms of these “base variables” $\\varepsilon_i$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathcal{F}}(\\lambda ; x)=\\frac{1}{n} \\sum_{i=1}^{n} \\log p\\left(x, \\sigma \\varepsilon_{i}+\\mu\\right)+H(q ; \\lambda)\n",
    "\\end{equation}\n",
    "\n",
    "we can now easily differentiate this expression with respect to $\\mu$ and $\\sigma$ (by the chain rule, this will involve the model gradient $\\nabla_z \\log{p(x,z)}$). The result is a stochastic estimate of the gradient of the ELBO, which you can plug into your favorite stochastic optimization algorithm (SGD, Adagrad, etc.).\n",
    "\n",
    "Note the only assumption we’ve made about the model is that we have access to gradients $\\nabla_z \\log{p(x,z)}$, which is nearly always the case thanks to automatic differentiation. This is how Stan implements variational inference for arbitrary models (more details in [their paper](https://arxiv.org/abs/1506.03431)), and many other languages now support autodiff as well, such as [autograd](https://github.com/HIPS/autograd) and [JAX](https://github.com/google/jax). \n",
    "\n",
    "If model gradients are not available, it’s still possible to estimate the ELBO gradient using a trick from reinforcement learning, described in the paper [Black Box Variational Inference](https://arxiv.org/abs/1401.0118). However, this estimate is higher-variance, so optimization will converge much more slowly than when model gradients are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$^1$There are other approaches, including the alternate divergence $KL[p‖q]$ which leads to [expectation propagation](https://tminka.github.io/papers/ep/), or the Laplace approximation which locally matches the curvature at the mode. \n",
    "\n",
    "$^2$For certain classes of models, e.g., exponential families with conjugate priors, the math is well understood and essentially automateable. This is the idea behind [variational message](http://www.jmlr.org/papers/volume6/winn05a/winn05a.pdf) passing as implemented in, e.g., [Infer.NET](https://dotnet.github.io/infer/). \n",
    "\n",
    "$^3$This trick was introduced by Kingma, Salimans, and Welling in the context of variational autoencoders, though also independently proposed by several others around the same time. Shakir Mohamed has a nice post that goes into more depth on the history and applicability of this trick. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
