{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Expectation Maximisation\n",
    "\n",
    "## By Andr√© van Schaik\n",
    "\n",
    "### __[International Centre for Neuromorphic Systems](https://westernsydney.edu.au/icns)__        \n",
    "10/01/2019 - 29/01/2023 (yes, it has taken me 4 years with lots of interruptions!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made this notebook on Dynamic Expectation Maximisation (DEM) following \"Hierarchical Models in the Brain\", by Karl Friston, PLoS Computational Biology 4(11): e1000211. [doi:10.1371/journal.pcbi.1000211](https://doi.org/10.1371/journal.pcbi.1000211) also using \"DEM: A variational treatment of dynamic systems\" by Friston, Trujillo-Barreto, and Daunizeau, Neuroimage Volume 41, Issue 3, 1 July 2008, Pages 849-885 [doi:10.1016/j.neuroimage.2008.02.054](https://doi.org/10.1016/j.neuroimage.2008.02.054). I found some of the steps in those papers not obvious and couldn't find an obvious source that derived them in sufficient detail. I am hoping that I have now captured all these steps in this notebook and the ones it links to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic model in generalised coordinates ###\n",
    "\n",
    "We can write a dynamic input-state-output model as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde y &= \\tilde g + \\tilde z \\\\\n",
    "D \\tilde x &= \\tilde f + \\tilde w \\tag{1}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\tilde y$ are the observations, and $\\tilde x$ are the hidden states of the system. The $\\tilde a$ notation indicates variables and functions in generalised coordinates of motion $\\tilde a = [a, \\dot{a}, \\ddot{a}, \\dddot{a}, ...]^T$. Here, $\\dot{a}$ is the *value* of the derivative of $a$ with respect to time; in other words, it is a dimensionless number, even though the time derivative obviously has a unit of $[s^{-1}]$. One way of interpreting this is that $\\dot{a} = \\tau\\,da/dt$ where $\\tau = dt$ and all time is measured in units of $dt$, and similarly for the higher orders of motion.\n",
    "\n",
    "$D$ is a block-matrix derivative operator, whose first leading-diagonal contains identity matrices. This operator simply shifts the vectors of generalised motion so $a[i]$ that is replaced by $a[i+1]$.\n",
    "\n",
    "The predicted sensor response $\\tilde g$ and motion $\\tilde f$ of the hidden states $\\tilde x$ in absence of random fluctuations are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "g &= g(x, \\nu) \\\\\n",
    "\\dot{g} &= g_x \\dot{x} + g_\\nu \\dot{\\nu} \\\\\n",
    "\\ddot{g} &= g_x \\ddot{x} + g_\\nu \\ddot{\\nu} \\\\\n",
    "&\\phantom{g=\\,} \\vdots \\\\\n",
    "\\end{split}\n",
    "\\:\\:\\:\n",
    "\\begin{split}\n",
    "f &= f(x, \\nu) \\\\\n",
    "\\dot{f} &= f_x \\dot{x} + f_\\nu \\dot{\\nu} \\\\\n",
    "\\ddot{f} &= f_x \\ddot{x} + f_\\nu \\ddot{\\nu} \\\\\n",
    "&\\phantom{f=\\,} \\vdots \\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "\n",
    "Here, $f$ and $g$ are continuous nonlinear functions and $\\tilde \\nu$ are causes or inputs, which can also result from actions by the agent. The notation $a_b$ is shorthand for $\\partial{a}/\\partial{b}$. We assume that the observation noise $\\tilde z$ follows a zero-mean Gaussian distribution $p(\\tilde z) = \\mathcal{N}(0, \\tilde \\Sigma^z)$ and similarly for the state noise $p(\\tilde w) = \\mathcal{N}(0, \\tilde \\Sigma^w)$. The input drive is also Gaussian but with a mean that can be different from zero: $p(\\tilde \\nu) = N(\\tilde \\eta^\\nu, \\tilde C^\\nu)$, where we use $\\tilde C^\\nu$ instead of $\\tilde \\Sigma^\\nu$ to indicate this is a *prior* covariance, rather than a *conditional* covariance. We can then evaluate the joint density over observations $\\tilde y$, hidden states $\\tilde x$, and inputs $\\tilde \\nu$:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\tilde y, \\tilde x, \\tilde \\nu \\,|\\, \\theta, \\lambda) &= p(\\tilde y \\,|\\, \\tilde x, \\tilde \\nu, \\theta, \\lambda) \\; p(\\tilde x \\,|\\, \\tilde \\nu, \\theta, \\lambda) \\; p(\\tilde \\nu) \\\\\n",
    "p(\\tilde y \\,|\\, \\tilde x, \\tilde \\nu, \\theta, \\lambda) &= \\mathcal{N}(\\tilde y : \\tilde g, \\tilde \\Sigma(\\lambda)^z) \\\\\n",
    "p(\\tilde x \\,|\\, \\tilde \\nu, \\theta, \\lambda) &= \\mathcal{N}(D\\tilde x : \\tilde f, \\tilde \\Sigma(\\lambda)^w) \\\\\n",
    "p(\\tilde \\nu) &= \\mathcal{N}(\\tilde \\nu : \\eta^\\nu, C^\\nu)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\theta$ contains the parameters describing $f$ and $g$, and $\\lambda$ are hyperparameters which control the amplitude and smoothness of the random fluctuations. Here we have indicated explicitly which random variable is generated by each normal distribution. According to $(1)$, the random variable for state transition is $D\\tilde x$, which therefore links different levels of motion.\n",
    "\n",
    "Finally, we also assume Gaussian priors for the hyperparameters $\\lambda$ and $\\theta$:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\lambda) &= \\mathcal{N}(\\lambda : \\eta^\\lambda, C^\\lambda)\\\\\n",
    "p(\\theta) &= \\mathcal{N}(\\theta : \\eta^\\theta, C^\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to write the directed Bayesian graph for the model:\n",
    "\n",
    "<img src=\"DM.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inversion ###\n",
    "\n",
    "For model inversion, we are trying to estimate the parameters $\\vartheta$ of a model given some observations $y$ and a model $m$ by maximising the conditional density $p(\\vartheta \\,|\\, \\tilde y, m)$. However, this density is in general not directly calculable as it involves normalising over all possible observations. *Variational Bayes* suggests a workaround by minimising the Kullback-Leibler divergence between what it believes the state of its environment is (encoded in a Recognition density $q(\\theta)$) and the true Bayesian posterior.\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(\\: q(\\vartheta) \\; || \\; p(\\vartheta \\,|\\, \\tilde y, m) \\: ) = \\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta\\,|\\, \\tilde y, m)} \\: d\\vartheta}\n",
    "\\end{align*}\n",
    "\n",
    "The KL divergence is a measure of the difference between two probability distributions, is always positve, and is 0 if and only if the two distributions are the same. Thus adapting $q(\\vartheta)$ to minimise this KL divergence will result in $q(\\vartheta)$ being a close approximation of $p(\\vartheta\\,|\\, \\tilde y, m)$. \n",
    "\n",
    "Obviously, to evaluate this KL divergence directly, we would still need to be able to calculate $p(\\vartheta\\,|\\, \\tilde y, m)$ and we seem to have made no progress. However, the FEP uses the fact that $p(\\vartheta, \\tilde y, m) = p(\\vartheta\\,|\\, \\tilde y, m) p(\\tilde y, m)$, to write this as:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(\\: q(\\vartheta) \\; || \\; p(\\vartheta\\,|\\, \\tilde y, m) \\: ) &= \\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\tilde y, m)/p(\\tilde y, m)} \\: d\\vartheta} \\\\\n",
    "&= \\int{q(\\vartheta) \\: \\{ \\ln\\:q(\\vartheta) - \\ln\\:p(\\vartheta, \\tilde y, m) + \\ln\\:p(\\tilde y, m) \\} \\: d\\vartheta} \\\\\n",
    "&= \\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\tilde y, m)} \\: d\\vartheta} + \\int{q(\\vartheta) \\: \\ln p(\\tilde y, m) \\: d\\vartheta} \\\\\n",
    "&= \\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\tilde y, m)} \\: d\\vartheta} + \\ln p(\\tilde y, m) \\int{q(\\vartheta) \\: d\\vartheta} \\\\\n",
    "&= \\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\tilde y, m)} \\: d\\vartheta} + \\ln p(\\tilde y, m) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "since $\\int{q(\\vartheta) \\: d\\vartheta} = 1$ by definition of a probability density. We continue by writing:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(\\: q(\\vartheta) \\; || \\; p(\\vartheta\\,|\\, \\tilde y, m) \\: ) &= \\ln p(\\tilde y, m) - F\\\\\n",
    "F &= -\\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\tilde y, m)} \\: d\\vartheta} \\\\\n",
    "&= -D_{KL}(\\: q(\\vartheta) \\; || \\; p(\\vartheta, \\tilde y, m) \\: )\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The joint density $p(\\vartheta,\\tilde y, m)$ is called the generative density, and represents the agent's belief in how the world works. It can be factorised into $p(\\vartheta,\\tilde y, m) = p(\\tilde y, \\vartheta, m) = p(\\tilde y \\,|\\, \\vartheta, m)\\:p(\\vartheta, m)$ where a prior $p(\\vartheta, m)$ encodes the agent's beliefs for the world states prior to new sensory input, and a likelihood $p(\\tilde y|\\vartheta, m)$ encodes how the agent's sensory signals relate to the world states. Thus, if we have a model for how the world states generate sensory perception (or if we can learn one), we can calculate $F$, which is called the *Variational Free Energy*, and is the negative of the KL divergence between the Recognition density, $q(\\vartheta)$, and the Generative density, $p(\\vartheta, \\tilde y, m)$. We probably don't know $\\ln p(\\tilde y, m)$, but, since this doesn't depend on $\\vartheta$, it plays no role in optimising $q(\\vartheta)$. We can simply maximise $F$ to make $q(\\vartheta)$ the best possible approximation of $p(\\vartheta,\\tilde y, m)$, and thereby also of $p(\\vartheta\\,|\\, \\tilde y, m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a model, we can determine the probability of the observations under the model by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln p(\\tilde y, m) &= \\ln p(\\tilde y \\,|\\, m) + \\ln p(m) \\\\\n",
    "\\ln p(\\tilde y \\,|\\, m) &= \\ln p(\\tilde y, m) - \\ln p(m) \\\\\n",
    "&= F + D_{KL}(\\: q(\\vartheta) \\; || \\; p(\\vartheta\\,|\\, \\tilde y, m) \\: ) - \\ln p(m)\n",
    "\\end{align*}\n",
    "\n",
    "Thus for a given model ($p(m)=1$), we can write its log likelihood as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln p(\\tilde y \\,|\\, m) &= F + D_{KL}(\\: q(\\vartheta) \\; || \\; p(\\vartheta\\,|\\, \\tilde y, m) \\: )\n",
    "\\end{align*}\n",
    "\n",
    "This indicates that $F$ can be used as a lower-bound for the log-evidence, since the KL divergence term is always positive and is $0$ if and only if $q(\\vartheta) =  p(\\vartheta\\,|\\, \\tilde y, m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F$ can also be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "F &= -\\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\tilde y, m)} \\: d\\vartheta} \\\\\n",
    "&= \\left< \\ln p(\\vartheta, \\tilde y, m) \\right>_q - \\left< \\ln q(\\vartheta) \\right>_q\n",
    "\\end{align*}\n",
    "\n",
    "which comprises the internal energy $U(\\vartheta, \\tilde y) = \\ln p(\\vartheta, \\tilde y)$ of a given model $m$ expected under $q(\\vartheta)$ and the entropy of $q(\\vartheta)$, which is a measure of its uncertainty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation ###\n",
    "\n",
    "The introduction of $q(\\vartheta)$ converts the difficult integration problem inherent in Bayesian Inference into a much simpler optimisation problem of adapting $q(\\vartheta)$ to maximise $F$. To further simplify calculation, we usually assume that the model parameters can be partitioned over the states $u = [\\tilde \\nu, \\tilde x]^T$, the parameters $\\theta$, and the hyperparameters $\\lambda$, as:\n",
    "\n",
    "\\begin{align*}\n",
    "q(\\vartheta) &= q(u(t)) \\, q(\\theta) \\, q(\\lambda) \\\\\n",
    "&= \\prod_i q(\\vartheta^i) \\\\\n",
    "\\vartheta^i &= \\{u(t), \\theta, \\lambda\\}\n",
    "\\end{align*}\n",
    "\n",
    "This partition is called the *mean field* approximation in statistical physics. We further assume that over the timescale of inference, only the states $u$ change with time $t$, while the (hyper)parameters are assumed constant.\n",
    "\n",
    "Under this partition, optimisation is still achieved by maximising the Free Energy, but we can now do this separately for each partition, by averaging over the other partitions. To show this, we define $F$ as an integral over the parameter partitions:\n",
    "\n",
    "\\begin{align*}\n",
    "F &= \\int f^i \\, d\\vartheta^i \\\\\n",
    "&= \\int{q(\\vartheta) \\: \\ln p(\\vartheta, \\tilde y, m) \\: d\\vartheta} - \\int{q(\\vartheta) \\: \\ln q(\\vartheta) \\: d\\vartheta} \\\\\n",
    "&= \\iint{q(\\vartheta^i) \\: q(\\vartheta^{\\backslash i}) \\: \\ln p(\\vartheta, \\tilde y, m) \\: d\\vartheta^{\\backslash i} \\: d\\vartheta^i} - \\iint{q(\\vartheta^i) \\: q(\\vartheta^{\\backslash i}) \\: \\ln q(\\vartheta) \\: d\\vartheta^{\\backslash i} \\: d\\vartheta^i} \\\\\n",
    "&= \\int{\\left( \\int{q(\\vartheta^i) \\: q(\\vartheta^{\\backslash i}) \\: \\ln p(\\vartheta, \\tilde y, m) \\: d\\vartheta^{\\backslash i} } - \\int{q(\\vartheta^i) \\: q(\\vartheta^{\\backslash i}) \\: \\ln q(\\vartheta) \\: d\\vartheta^{\\backslash i} } \\right) \\: d\\vartheta^i }\\\\\n",
    "f^i &= \\int{q(\\vartheta^i) \\: q(\\vartheta^{\\backslash i}) \\: \\ln p(\\vartheta, \\tilde y, m) \\: d\\vartheta^{\\backslash i} } - \\int{q(\\vartheta^i) \\: q(\\vartheta^{\\backslash i}) \\: \\ln q(\\vartheta) \\: d\\vartheta^{\\backslash i}} \\\\\n",
    "&= q(\\vartheta^i) \\: \\int{ q(\\vartheta^{\\backslash i}) \\: U(\\vartheta, \\tilde y) \\: d\\vartheta^{\\backslash i} } - q(\\vartheta^i) \\: \\int{q(\\vartheta^{\\backslash i}) \\: (\\ln q(\\vartheta^i) + \\ln q(\\vartheta^{\\backslash i})) \\: d\\vartheta^{\\backslash i}} \\\\\n",
    "&= q(\\vartheta^i) \\left( V(\\vartheta^i) - \\ln q(\\vartheta^i) - \\int{q(\\vartheta^{\\backslash i}) \\: \\ln q(\\vartheta^{\\backslash i}) \\: d\\vartheta^{\\backslash i}} \\right) \\\\\n",
    "F &= \\int q(\\vartheta^i) \\: \\left( V(\\vartheta^i) - \\ln q(\\vartheta^i) - \\ln Z^i \\right)\\, d\\vartheta^i \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\vartheta^{\\backslash i}$ denotes all parameters not in set $i$, i.e., its Markov blanket,  $\\ln Z^i$ contains all the terms of $f^i$ that do not depend on $\\vartheta^i$, and\n",
    "\n",
    "\\begin{align*}\n",
    "V(\\vartheta^i) &= \\int{q(\\vartheta^{\\backslash i}) \\: \\ln p(\\vartheta, \\tilde y, m) \\: d\\vartheta^{\\backslash i} } = \\int{ q(\\vartheta^{\\backslash i}) \\: U(\\vartheta, \\tilde y) \\: d\\vartheta^{\\backslash i} } = \\left< U(\\vartheta) \\right>_{q(\\vartheta^{\\backslash i})}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The free energy is maximised when the derivative of $F$ with respect to the distribution $q(\\vartheta^i)$ = 0. \n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{q(\\vartheta^i)} F &= q(\\vartheta^i) \\: \\left( V(\\vartheta^i) - \\ln q(\\vartheta^i) - \\ln Z^i \\right) = 0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since this has to hold for any choice of $q(\\vartheta^i)$, this means that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( V(\\vartheta^i) - \\ln q(\\vartheta^i) - \\ln Z^i \\right) &= 0 \\\\\n",
    "\\ln q(\\vartheta^i) &= V(\\vartheta^i) - \\ln Z^i\\\\\n",
    "q(\\vartheta^i) &= \\frac{1}{Z^i} \\exp \\left(V(\\vartheta^i)\\right) = \\frac{1}{Z^i} \\exp\\left(\\left< U(\\vartheta) \\right>_{q(\\vartheta^{\\backslash i})}\\right) \n",
    "\\end{align*}\n",
    "\n",
    "Thus, $Z^i$ is a normalisation constant ensuring the distribtion integrates to $1$, and is also called a partition function in physics. The final equation indicates that the variational density over one parameter set is an exponential function of the internal energy averaged over all other parameters.\n",
    "\n",
    "Given our partitions above, we can then write:\n",
    "\n",
    "\\begin{align*}\n",
    "q(u(t)) &\\propto \\exp \\left(V(t)\\right) \\\\\n",
    "V(t) &= \\left< U(t) \\right>_{q(\\theta)q(\\lambda)} \\\\\n",
    "q(\\theta) &\\propto \\exp \\left(\\bar{V}^\\theta \\right) \\\\\n",
    "\\bar{V}^\\theta &= \\int \\left< U(t) \\right>_{q(u)q(\\lambda)} \\:dt + U^\\theta \\\\\n",
    "q(\\lambda) &\\propto \\exp \\left(\\bar{V}^\\lambda \\right) \\\\\n",
    "\\bar{V}^\\lambda &= \\int \\left< U(t) \\right>_{q(u)q(\\theta)} \\:dt + U^\\lambda \\\\\n",
    "\\bar{U} &= \\int U(t)\\:dt + U^\\theta + U^\\lambda \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In a dynamical system, the instantaneous internal energy $U(t)$ is a function of time. Because the parameters and hyperparameters are considered constant over a period of observation, their variational densities are functions of the path integal of this internal energy, which is called *action* in physics. We use the notation $\\bar{V}$ and $\\bar{U}$ to indicate these integrals. $U^\\theta = \\ln p(\\theta)$ and $U^\\lambda = \\ln p(\\lambda)$ are the prior energies of the parameters and hyperparameters, respectively. \n",
    "\n",
    "From these equations we see that the variational density over states can be determined from the instantaneous internal energy averaged over parameters and hyperparameters, whereas the density over parameters and hyperparameters can only be determined when data has been observed over a certain amount of time. In the absence of data, the integrals will be zero, and the conditional density simply reduces to the prior density.\n",
    "\n",
    "*Variational Bayes* assumes the above equations are analytically tractable, which needs the choice of appropriate (conjugate) priors. The conditional distributions $q(\\vartheta^i)$ above can then be updated through iteration as new data becomes available:\n",
    "\n",
    "\\begin{align*}\n",
    "U(t) &= \\ln p(\\vartheta, \\tilde y, m)(t) \\\\\n",
    "\\ln q(u(t)) &\\propto \\left< U(t) \\right>_{q(\\theta)q(\\lambda)} \\\\\n",
    "\\ln q(\\theta) &\\propto \\int \\left< U(t) \\right>_{q(u)q(\\lambda)} \\:dt + \\ln p(\\theta) \\\\\n",
    "\\ln q(\\lambda) &\\propto \\int \\left< U(t) \\right>_{q(u)q(\\theta)} \\:dt + \\ln p(\\lambda)\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Laplace approximation ###\n",
    "\n",
    "If the above equations are not analytically tractable, or if we want to avoid those calculations, we can apply the Laplace approximation. The Laplace approximation assumes that the marginals of the conditional density assume a Gaussian form, i.e., $q(\\vartheta^i) = \\mathcal{N}(\\vartheta^i : \\mu^i, \\Sigma^i)$, where $\\mu^i$ and $\\Sigma^i$ are the sufficient statistics. For notational clarity, we will use $\\mu^i$,  $\\Sigma^i$, and $\\Pi^i$ for the conditional mean, covariance, and precision of the $i^\\text{th}$ marginal, respectively, and $\\eta^i$,  $C^i$, and $P^i$ for their priors. This approximation simplifies the updates to the marginals of the conditional densities.\n",
    "\n",
    "For each partition $\\vartheta^i$, we can then write:\n",
    "\n",
    "\\begin{align*}\n",
    "q(\\vartheta^i) &= \\frac{1}{\\sqrt{(2\\pi)^{n^i} |\\Sigma^i|}} \\exp \\left( \\frac{-(\\vartheta^i - \\mu^i)^2}{2\\Sigma^i} \\right) \\\\\n",
    "&= \\frac{1}{Z^i} \\exp \\left( -\\varepsilon(\\vartheta^i) \\right) \\\\ \n",
    "Z^i &= \\sqrt{(2\\pi)^{n^i} |\\Sigma^i|} \\\\\n",
    "\\varepsilon(\\vartheta^i) &= \\frac{(\\vartheta^i - \\mu^i)^2}{2\\Sigma^i} \\\\\n",
    "&= \\frac{1}{2} (\\vartheta^i - \\mu^i)^T {\\Sigma^i}^{-1} (\\vartheta^i - \\mu^i)\n",
    "\\end{align*}\n",
    "\n",
    "Where $n^i$ is the number of parameters in partition $i$.\n",
    "\n",
    "Recall that the Free Energy was defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "F &= -\\int{q(\\vartheta) \\: \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\tilde y, m)} \\: d\\vartheta} \\\\\n",
    "&= - \\int{q(\\vartheta) \\: \\ln q(\\vartheta) \\: d\\vartheta} + \\int{q(\\vartheta) \\: \\ln p(\\vartheta, \\tilde y, m) \\: d\\vartheta} \\\\\n",
    "&= - \\int{q(\\vartheta) \\: \\ln \\prod_i q(\\vartheta^i) \\: d\\vartheta} + \\left< U \\right>_q \\\\\n",
    "&= - \\int{q(\\vartheta) \\: \\ln \\prod_i \\frac{1}{Z^i} \\exp \\left( -\\varepsilon(\\vartheta^i) \\right) \\: d\\vartheta} + \\left< U \\right>_q \\\\\n",
    "&= \\int{q(\\vartheta) \\: \\sum_i(\\ln Z^i + \\varepsilon(\\vartheta^i)) \\: d\\vartheta} + \\left< U \\right>_q \\\\\n",
    "&= \\sum_i(\\ln Z^i)\\int{q(\\vartheta) \\: d\\vartheta} + \\int{q(\\vartheta) \\: \\sum_i(\\varepsilon(\\vartheta^i)) \\: d\\vartheta} + \\left< U \\right>_q \\\\\n",
    "&\\left(\\int{q(\\vartheta) \\: d\\vartheta} = 1\\right) \\\\\n",
    "&= \\sum_i(\\ln Z^i) + \\int{q(\\vartheta) \\: \\sum_i(\\frac{1}{2} (\\vartheta^i - \\mu^i)^T {\\Sigma^i}^{-1} (\\vartheta^i - \\mu^i)) \\: d\\vartheta} + \\left< U \\right>_q \\\\\n",
    "&= \\sum_i(\\ln Z^i) + \\sum_i(\\frac{1}{2} \\int{q(\\vartheta^i) \\: (\\vartheta^i - \\mu^i)^T {\\Sigma^i}^{-1} (\\vartheta^i - \\mu^i) \\: d\\vartheta^i}) + \\left< U \\right>_q \\\\\n",
    "&\\left( \\text{using } a^T B a = \\text{tr} \\left( a a^T B \\right) \\right)\\\\\n",
    "&= \\sum_i(\\ln Z^i) + \\sum_i \\frac{1}{2}\\int{q(\\vartheta^i) \\: \\text{tr} \\left((\\vartheta^i - \\mu^i) (\\vartheta^i - \\mu^i)^T  {\\Sigma^i}^{-1} \\right) \\: d\\vartheta^i}) + \\left< U \\right>_q \\\\\n",
    "&= \\sum_i(\\ln Z^i) + \\sum_i \\frac{1}{2} \\int{\\text{tr} \\left(q(\\vartheta^i) \\: (\\vartheta^i - \\mu^i) (\\vartheta^i - \\mu^i)^T  {\\Sigma^i}^{-1} \\right) \\: d\\vartheta^i}) + \\left< U \\right>_q \\\\\n",
    "&= \\sum_i(\\ln Z^i) + \\sum_i \\frac{1}{2} \\text{tr} \\left(\\int{q(\\vartheta^i) \\: (\\vartheta^i - \\mu^i) (\\vartheta^i - \\mu^i)^T  {\\Sigma^i}^{-1} \\: d\\vartheta^i}) \\right) + \\left< U \\right>_q \\\\\n",
    "&= \\sum_i(\\ln Z^i) + \\sum_i \\frac{1}{2} \\text{tr} \\left(\\Sigma^i {\\Sigma^i}^{-1} \\right) + \\left< U \\right>_q \\\\\n",
    "&= \\sum_i(\\ln Z^i + \\frac{n^i}{2}) + \\left< U \\right>_q \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we still need to find an expression we can calculate for $\\left< U \\right>_q$. To do this, a further approximation assumes that $q$ is sharply peaked at its mean value $\\mu$, so that the integration is only non-zero close to $\\vartheta = \\mu$. This seems quite restrictive: not only do we assume $q$ is a Gaussian distribution, but also it has to be a narrow distribution around its mean. However, this is just another way of saying that the parameters are nearly constant over the time segment that we are doing inference over. With this assumption we can then use a Taylor expansion around the mean up to second order to obtain: \n",
    "\n",
    "\\begin{align*}\n",
    "\\left< U \\right>_q &= \\int{q(\\vartheta) \\: U(\\vartheta, \\tilde y) \\: d\\vartheta} \\\\\n",
    "&= \\int{q(\\vartheta) \\: \\left\\{ U(\\mu, \\tilde y) + \\left[ \\frac{dU}{d\\vartheta} \\right]_\\mu \\delta\\vartheta + \\frac{1}{2} \\left[ \\frac{d^2U}{d\\vartheta^2} \\right]_\\mu \\delta\\vartheta^2 \\right\\} \\: d\\vartheta} \\\\\n",
    "&= U(\\mu, \\tilde y) + \\left[ \\frac{dU}{d\\vartheta} \\right]_\\mu \\int{q(\\vartheta) \\: (\\vartheta - \\mu) \\: d\\vartheta} + \\frac{1}{2} \\int{ q(\\vartheta) \\: (\\vartheta - \\mu)^T \\left[ \\frac{d^2U}{d\\vartheta^2} \\right]_\\mu (\\vartheta - \\mu) \\: d\\vartheta} \\\\\n",
    "&= U(\\mu, \\tilde y) + \\left[ \\frac{dU}{d\\vartheta} \\right]_\\mu \\left\\{ \\int{\\vartheta q(\\vartheta) \\: d\\vartheta} - \\mu \\right\\} + \\frac{1}{2} \\int{ q(\\vartheta) \\: \\text{tr} \\left( (\\vartheta - \\mu) (\\vartheta - \\mu)^T \\left[ \\frac{d^2U}{d\\vartheta^2} \\right]_\\mu \\right) \\: d\\vartheta} \\\\\n",
    "&= U(\\mu, \\tilde y) + \\left[ \\frac{dU}{d\\vartheta} \\right]_\\mu \\left\\{ \\int{\\vartheta q(\\vartheta) \\: d\\vartheta} - \\mu \\right\\} + \\frac{1}{2} \\text{tr} \\left( \\left[ \\frac{d^2U}{d\\vartheta^2} \\right]_\\mu \\int{ q(\\vartheta) \\: (\\vartheta - \\mu) (\\vartheta - \\mu)^T \\: d\\vartheta} \\right) \\\\\n",
    "&= U(\\mu, \\tilde y) + \\left[ \\frac{dU}{d\\vartheta} \\right]_\\mu \\left\\{ \\mu - \\mu \\right\\} + \\frac{1}{2} \\text{tr} \\left( \\left[ \\frac{d^2U}{d\\vartheta^2} \\right]_\\mu \\Sigma \\right) \\\\\n",
    "&= U(\\mu, \\tilde y) + \\frac{1}{2} \\text{tr} \\left( \\left[ \\frac{d^2U}{d\\vartheta^2} \\right]_\\mu \\Sigma \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This now allows us to write for the free energy:\n",
    "\n",
    "\\begin{align*}\n",
    "F &= \\sum_i\\left(\\ln Z^i + \\frac{n^i}{2}\\right) + \\left< U \\right>_q \\\\\n",
    "&= \\sum_i\\left(\\frac{1}{2} \\ln (2\\pi)^{n^i} |\\Sigma^i| + \\frac{n^i}{2}\\right)  + U(\\mu, \\tilde y) + \\frac{1}{2} \\sum_i \\text{tr} \\left( \\left[ \\frac{d^2U}{d{\\vartheta^i}^2} \\right]_{\\mu^i} \\Sigma^i \\right)\\\\\n",
    "&= \\frac{1}{2} \\sum_i\\left(n^i \\ln (2\\pi) +  \\ln |\\Sigma^i| + n^i \\right)  + U(\\mu, \\tilde y) + \\frac{1}{2} \\sum_i \\left\\{ \\text{tr} \\left( \\left[ \\frac{d^2U}{d{\\vartheta^i}^2} \\right]_{\\mu^i} \\Sigma^i \\right) \\right\\} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "To find the optimal variances, we maximise the free energy with respect to the variances, so that the partial derivatives are zero:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dF}{d\\Sigma^i} &= \\frac{1}{2} \\left\\{ \\left[ \\frac{d^2U}{d{\\vartheta^i}^2} \\right]_{\\mu^i} + {\\Sigma^i}^{-1} \\right\\}^T = 0 \\\\\n",
    "\\implies \\Sigma^{i*} &=  - \\left[ \\frac{d^2U}{d{\\vartheta^i}^2} \\right]_{\\mu^i}^{-1} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where we've used the matrix derivative identities:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d \\text{tr} \\left( B A \\right)}{dA} &:= B^T \\\\\n",
    "\\frac{d \\ln |A|}{dA} &:= {{A}^{-1}}^T \\\\\n",
    "\\end{align*}\n",
    "\n",
    "and we use the notation $\\Sigma^{i*}$ to indicate this is the optimal variance which maximises the free energy.\n",
    "\n",
    "Finally, this allows us to write for the free energy under the Laplace approximation with sharply peaked Gaussian distributions for $q(\\vartheta^i)$:\n",
    "\n",
    "\\begin{align*}\n",
    "F &= \\frac{1}{2} \\sum_i\\left(n^i \\ln (2\\pi) + \\ln |\\Sigma^{i*}| + n^i\\right)  + U(\\mu, \\tilde y) + \\frac{1}{2} \\sum_i \\text{tr}\\left(- \\left[ \\frac{d^2U}{d{\\vartheta^i}^2} \\right]_{\\mu^i} \\left[ \\frac{d^2U}{d{\\vartheta^i}^2} \\right]_{\\mu^i}^{-1}\\right)\\\\\n",
    "&= \\frac{1}{2} \\sum_i\\left(n^i \\ln (2\\pi) +\\ln |\\Sigma^{i*}| + n^i\\right)  + U(\\mu, \\tilde y) + \\frac{1}{2} \\sum_i \\text{tr}\\left(-I\\right)\\\\\n",
    "&= \\frac{1}{2} \\sum_i\\left(n^i \\ln (2\\pi) + \\ln |\\Sigma^{i*}| + n^i\\right)  + U(\\mu, \\tilde y) - \\frac{n^i}{2} \\\\\n",
    "&= U(\\mu, \\tilde y) + \\frac{1}{2} \\sum_i \\left( n^i \\ln (2\\pi) + \\ln |\\Sigma^{i*}| \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to obtain an expression for the variational distribution of each partition under the Laplace approximation.\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln q(u(t)) &\\propto V(t) = \\left< U(t) \\right>_{q(\\theta)q(\\lambda)}\\\\\n",
    "\\ln q(\\theta) &\\propto \\bar{V}^\\theta = \\int \\left< U(t) \\right>_{q(u)q(\\lambda)} \\:dt + U^\\theta\\\\\n",
    "\\ln q(\\lambda) &\\propto \\bar{V}^\\lambda = \\int \\left< U(t) \\right>_{q(u)q(\\theta)} \\:dt + U^\\lambda\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Because of the mean field assumption, we can also write:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln p(\\vartheta, \\tilde y, m) &= \\ln p(u, t, \\tilde y, m) + \\ln p(\\theta, \\tilde y, m) + \\ln p(\\lambda, \\tilde y, m)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "and we have from before our estimates for these distributions:\n",
    "\n",
    "\\begin{align*}\n",
    "q(\\vartheta^i) &= \\frac{1}{Z^i} \\exp \\left( \\frac{-(\\vartheta^i - \\mu^i)^2}{2\\Sigma^i} \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now this allows us to proceed with:\n",
    "\n",
    "\\begin{align*}\n",
    "V(t) &= \\left< U(t) \\right>_{q(\\theta)q(\\lambda)} \\\\\n",
    "&= \\int q(\\lambda) q(\\theta) \\ln p(\\vartheta, \\tilde y, m) d\\theta d\\lambda \\\\\n",
    "&= \\int q(\\lambda) q(\\theta) \\ln p(u, t, \\tilde y, m) + \\ln p(\\theta, \\tilde y, m) + \\ln p(\\lambda, \\tilde y, m) d\\theta d\\lambda \\\\\n",
    "&\\approx \\int q(\\lambda) q(\\theta) \\ln p(u, t, \\tilde y, m) + \\ln q(\\theta, \\tilde y, m) + \\ln q(\\lambda, \\tilde y, m) d\\theta d\\lambda \\\\\n",
    "&= \\int q(\\lambda) q(\\theta) \\ln \\left( p(u,t|\\mu^\\theta, \\mu^\\lambda) \\right) d\\theta d\\lambda \\\\\n",
    "&+ \\int q(\\lambda) q(\\theta) \\left( -\\frac{1}{2} \\left( \\theta - \\mu^{\\theta} \\right)^T {\\Sigma^{\\theta}}^{-1} \\left( \\theta - \\mu^{\\theta} \\right) - \\ln Z^\\theta \\right) d\\theta d\\lambda \\\\\n",
    "&+ \\int q(\\lambda) q(\\theta) \\left( -\\frac{1}{2} \\left( \\lambda - \\mu^{\\lambda} \\right)^T {\\Sigma^{\\lambda}}^{-1} \\left( \\lambda - \\mu^{\\lambda} \\right)  - \\ln Z^\\lambda \\right) d\\theta d\\lambda \\\\\n",
    "&= U(u,t|\\mu^\\theta, \\mu^\\lambda) \\\\\n",
    "&+ \\int q(\\lambda) q(\\theta) \\left( -\\frac{1}{2} \\left( \\theta - \\mu^{\\theta} \\right)^T {\\Sigma^{\\theta}}^{-1} \\left( \\theta - \\mu^{\\theta} \\right) \\right) d\\theta d\\lambda - \\int q(\\theta) \\ln Z^\\theta d\\theta\\\\\n",
    "&+ \\int q(\\lambda) q(\\theta) \\left( -\\frac{1}{2} \\left( \\lambda - \\mu^{\\lambda} \\right)^T {\\Sigma^{\\lambda}}^{-1} \\left( \\lambda - \\mu^{\\lambda} \\right) \\right) d\\theta d\\lambda - \\int q(\\lambda) \\ln Z^\\lambda d\\lambda\\\\\n",
    "&\\text{(inserting the optimal variances:)}\\\\\n",
    "&= U(u,t|\\mu^\\theta, \\mu^\\lambda) \\\\\n",
    "&+ \\int q(\\lambda) q(\\theta) \\left( \\frac{1}{2} \\left( \\theta - \\mu^{\\theta} \\right)^T \\left[ \\frac{d^2U(t)}{d{\\theta}^2} \\right]_{\\mu^\\theta} \\left( \\theta - \\mu^{\\theta} \\right) \\right) d\\theta d\\lambda - \\left< \\ln Z^\\theta \\right>_{q(\\theta)}\\\\\n",
    "&+ \\int q(\\lambda) q(\\theta) \\left( \\frac{1}{2} \\left( \\lambda - \\mu^{\\lambda} \\right)^T \\left[ \\frac{d^2U(t)}{d{\\lambda}^2} \\right]_{\\mu^\\lambda} \\left( \\lambda - \\mu^{\\lambda} \\right) \\right) d\\theta d\\lambda - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)}\\\\\n",
    "&\\left( \\text{and using } a^T B a = \\text{tr} \\left( a a^T B \\right) \\right)\\\\\n",
    "&= U(u,t|\\mu^\\theta, \\mu^\\lambda) \\\\\n",
    "&+ \\int q(\\theta) \\text{tr} \\left( \\frac{1}{2} \\left( \\theta - \\mu^{\\theta} \\right) \\left( \\theta - \\mu^{\\theta} \\right)^T \\left[ \\frac{d^2U(t)}{d{\\theta}^2} \\right]_{\\mu^\\theta} \\right) d\\theta - \\left< \\ln Z^\\theta \\right>_{q(\\theta)}\\\\\n",
    "&+ \\int q(\\lambda) \\text{tr} \\left( \\frac{1}{2} \\left( \\lambda - \\mu^{\\lambda} \\right) \\left( \\lambda - \\mu^{\\lambda} \\right)^T \\left[ \\frac{d^2U(t)}{d{\\lambda}^2} \\right]_{\\mu^\\lambda} \\right) d\\lambda - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)}\\\\\n",
    "&\\left( \\text{and the integal of a trace is the trace of the integral:} \\right)\\\\\n",
    "&= U(u,t|\\mu^\\theta, \\mu^\\lambda) \\\\\n",
    "&+ \\frac{1}{2} \\text{tr} \\left( \\int q(\\theta) \\left( \\theta - \\mu^{\\theta} \\right) \\left( \\theta - \\mu^{\\theta} \\right)^T \\left[ \\frac{d^2U(t)}{d{\\theta}^2} \\right]_{\\mu^\\theta} d\\theta \\right) - \\left< \\ln Z^\\theta \\right>_{q(\\theta)}\\\\\n",
    "&+  \\frac{1}{2} \\text{tr} \\left( \\int q(\\lambda)\\left( \\lambda - \\mu^{\\lambda} \\right) \\left( \\lambda - \\mu^{\\lambda} \\right)^T \\left[ \\frac{d^2U(t)}{d{\\lambda}^2} \\right]_{\\mu^\\lambda} d\\lambda \\right) - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)}\\\\\n",
    "&\\left( \\text{and because the optimal variances are constant when evaluating the integral} \\right)\\\\\n",
    "&= U(u,t|\\mu^\\theta, \\mu^\\lambda) \\\\\n",
    "&+ \\frac{1}{2} \\text{tr} \\left( \\int q(\\theta) \\left( \\theta - \\mu^{\\theta} \\right) \\left( \\theta - \\mu^{\\theta} \\right)^Td\\theta \\left[ \\frac{d^2U(t)}{d{\\theta}^2} \\right]_{\\mu^\\theta} \\right) - \\left< \\ln Z^\\theta \\right>_{q(\\theta)}\\\\\n",
    "&+  \\frac{1}{2} \\text{tr} \\left( \\int q(\\lambda)\\left( \\lambda - \\mu^{\\lambda} \\right) \\left( \\lambda - \\mu^{\\lambda} \\right)^T d\\lambda \\left[ \\frac{d^2U(t)}{d{\\lambda}^2} \\right]_{\\mu^\\lambda} \\right) - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)}\\\\\n",
    "&= U(u,t|\\mu^\\theta, \\mu^\\lambda) + \\frac{1}{2} \\text{tr}\\left( \\Sigma^\\theta \\left[ \\frac{d^2U(t)}{d{\\theta}^2} \\right]_{\\mu^\\theta} \\right) + \\frac{1}{2} \\text{tr} \\left(\\Sigma^\\lambda \\left[ \\frac{d^2U(t)}{d{\\lambda}^2} \\right]_{\\mu^\\lambda} \\right) - \\left< \\ln Z^\\theta \\right>_{q(\\theta)} - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)}\\\\\n",
    "\\bar{V}^u &= \\int V(t) \\:dt \\\\\n",
    "&= \\int U(u, t|\\mu^\\theta, \\mu^\\lambda) + W(t)^\\theta + W(t)^\\lambda - \\left< \\ln Z^\\theta \\right>_{q(\\theta)} - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)} \\:dt \\\\\n",
    "\\text{Similarly, we find:}\\\\\n",
    "\\bar{V}^\\theta &= \\int U(\\mu^u, t|\\theta, \\mu^\\lambda) + W(t)^u + W(t)^\\lambda - \\left< \\ln Z^u \\right>_{q(u)} - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)} \\:dt + U^\\theta \\\\\n",
    "\\bar{V}^\\lambda &= \\int U(\\mu^u, t|\\mu^\\theta, \\lambda) + W(t)^u + W(t)^\\theta - \\left< \\ln Z^u \\right>_{q(u)} - \\left< \\ln Z^\\theta \\right>_{q(\\theta)} \\:dt + U^\\lambda \\\\\n",
    "W(t)^u &= \\frac{1}{2} \\text{tr}(\\Sigma^u U(t)_{uu}) \\\\\n",
    "W(t)^\\theta &= \\frac{1}{2} \\text{tr}(\\Sigma^\\theta U(t)_{\\theta\\theta}) \\\\\n",
    "W(t)^\\lambda &= \\frac{1}{2} \\text{tr}(\\Sigma^\\lambda U(t)_{\\lambda\\lambda}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Also, the conditional precisions are equal to the negative curvatures of the internal action:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{U} &= \\int U(t)\\:dt + U^\\theta + U^\\lambda \\\\\n",
    "\\Pi^u &= -\\bar{U}_{uu} = -U(t)_{uu} \\\\\n",
    "\\Pi^\\theta &= -\\bar{U}_{\\theta\\theta} = - \\int U(t)_{\\theta\\theta} \\: dt - U^\\theta_{\\theta\\theta} \\\\\n",
    "\\Pi^\\lambda &= -\\bar{U}_{\\lambda\\lambda} = - \\int U(t)_{\\lambda\\lambda} \\: dt- U^\\lambda_{\\lambda\\lambda} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "To get their optimal values, these need to be evaluated at the mode of each partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal smoothness ###\n",
    "\n",
    "Since the different levels of motion in the generalised coordinates are linked, the actual precision matrix will have off-diagonal elements with non-zero values. The precision is given by the Kronecker product $\\tilde \\Pi^i = S(\\gamma) \\otimes \\Pi^i$, where $\\Pi^i$ is a diagonal matrix specifying the precision of the (often assumed independent) Gaussian noise at each level as determined in the previous section, and $S(\\gamma)$ is the temporal precision matrix, which encodes the temporal dependencies between levels, which is a function of their autocorrelations:\n",
    "\n",
    "\\begin{align*}\n",
    "S =\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & \\ddot{\\rho}(0) & 0 & \\ddot{\\ddot{\\rho}}(0) & 0 \\\\\n",
    "    0 & -\\ddot{\\rho}(0) & 0 & -\\ddot{\\ddot{\\rho}}(0) & 0 & -\\dddot{\\dddot{\\rho}}(0)\\\\\n",
    "    \\ddot{\\rho}(0) & 0 & \\ddot{\\ddot{\\rho}}(0) & 0 & \\dddot{\\dddot{\\rho}}(0) & 0 \\\\\n",
    "    0 & -\\ddot{\\ddot{\\rho}}(0) & 0 & -\\dddot{\\dddot{\\rho}}(0) & 0 & -\\ddot{\\dddot{\\dddot{\\rho}}}(0) \\\\\n",
    "    \\ddot{\\ddot{\\rho}}(0) & 0 & \\dddot{\\dddot{\\rho}}(0) & 0 & \\ddot{\\dddot{\\dddot{\\rho}}}(0) & 0 \\\\\n",
    "    0 & -\\dddot{\\dddot{\\rho}}(0) & 0 & -\\ddot{\\dddot{\\dddot{\\rho}}}(0) & 0 & -\\ddot{\\ddot{\\dddot{\\dddot{\\rho}}}}(0)\\\\\n",
    "    \\end{bmatrix}^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "(see [here](Generalised%20precision%20matrix.ipynb) for my derivation of this step.) Here $\\ddot{\\rho}(0)$ is the second derivative with respect to time of the autocorrelation function evaluated at zero. Note, that because the autocorrelation function is even (symmetrical for positive and negative delays), the odd derivatives of the autocorrelation function are all odd functions, and thus are zero when evaluated at zero.\n",
    "\n",
    "While $S$ can be evaluated for any analytical autocorrelation function, we assume here that the temporal correlations all have the same Gaussian form, which gives:\n",
    "\n",
    "\\begin{align*}\n",
    "S &=\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & -\\gamma & 0 & 3 \\gamma^2 & 0 \\\\\n",
    "    0 & \\gamma & 0 & -3 \\gamma^2 & 0 & 15 \\gamma^3 \\\\\n",
    "    -\\gamma & 0 & 3 \\gamma^2 & 0 & -15 \\gamma^3 & 0 \\\\\n",
    "    0 & -3 \\gamma^2 & 0 & 15 \\gamma^3 & 0 & -105 \\gamma^4 \\\\\n",
    "    3 \\gamma^2 & 0 & -15 \\gamma^3 & 0 & 105 \\gamma^4 & 0 \\\\\n",
    "    0 & 15 \\gamma^3 & 0 & -105 \\gamma^4 & 0 & 945 \\gamma^5 \\\\\n",
    "    \\end{bmatrix}^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\gamma$ is the precision parameter of a Gaussian autocorrelation function. Typically, $\\gamma > 1$, which ensures the precisions of high-order motion converge quickly to zero. This is important because it enables us to truncate the representation of an infinite number of generalised coordinates to a relatively small number, since high-order prediction errors have a vanishingly small precision. Friston states that an order of n=6 is sufficient in most cases. Also note that my derivation of this matrix compared to Friston has $\\gamma_{AvS} = 1/2 \\gamma_{KF}$. I believe this is because I took the Gaussian autocorrelation function with precision parameter $\\gamma$ to be $\\rho(h) = \\exp(-\\frac{\\gamma}{2} h^2)$, whereas Friston's result implies he used $\\rho(h) = \\exp(-\\frac{\\gamma}{4} h^2)$, which is an odd definition.\n",
    "\n",
    "Putting all this together, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\tilde y, \\tilde x, \\tilde \\nu \\,|\\, \\theta, \\lambda) &= p(\\tilde y \\,|\\, \\tilde x, \\tilde \\nu, \\theta, \\lambda) \\; p(\\tilde x \\,|\\, \\tilde \\nu, \\theta, \\lambda) \\; p(\\tilde \\nu) \\\\\n",
    " &= (2\\pi)^{-N_y/2} {|\\tilde\\Pi^z|}^{1/2} e^{-\\frac{1}{2}{\\tilde\\varepsilon^\\nu}^T \\tilde\\Pi^z \\tilde\\varepsilon^\\nu} (2\\pi)^{-N_x/2} {|\\tilde\\Pi^w|}^{1/2} e^{-\\frac{1}{2}{\\tilde\\varepsilon^x}^T \\tilde\\Pi^w \\tilde\\varepsilon^x} \\; p(\\tilde \\nu) \\\\\n",
    " &= (2\\pi)^{-(N_y + N_x)/2} (|\\tilde\\Pi^z| + |\\tilde\\Pi^w|)^{1/2} e^{-\\frac{1}{2}{\\tilde\\varepsilon^\\nu}^T \\tilde\\Pi^z \\tilde\\varepsilon^\\nu}  e^{-\\frac{1}{2}{\\tilde\\varepsilon^x}^T \\tilde\\Pi^w \\tilde\\varepsilon^x} \\; p(\\tilde \\nu)\\\\\n",
    " &= (2\\pi)^{-N/2} |\\tilde\\Pi|^{1/2} e^{-\\frac{1}{2}{\\tilde\\varepsilon}^T \\tilde\\Pi \\tilde\\varepsilon} \\; p(\\tilde \\nu)\\\\ \n",
    "\\tilde\\Pi &= \n",
    "    \\begin{bmatrix}\n",
    "    \\tilde\\Pi^z & \\\\\n",
    "    & \\tilde\\Pi^w\n",
    "    \\end{bmatrix}\\\\\n",
    "\\tilde\\varepsilon &= \n",
    "    \\begin{bmatrix}\n",
    "    \\tilde\\varepsilon^\\nu = \\tilde y - \\tilde g   \\\\\n",
    "    \\tilde\\varepsilon^x = D\\tilde x - \\tilde f\n",
    "    \\end{bmatrix}\\\\\n",
    "N &= \\text{Rank}(\\tilde\\Pi)\n",
    "\\end{align*} \n",
    "\n",
    "Here we introduce auxilary variables $\\tilde\\varepsilon(t)$, which are the prediction errors for the generalised responses and motion of the hidden states, with respective predictions $\\tilde g(t)$ and $\\tilde f(t)$ and their precisions encoded by $\\tilde\\Pi$.\n",
    "\n",
    "The log probability can thus be written:\n",
    "\\begin{align*}\n",
    "\\ln p(\\tilde y, \\tilde x, \\tilde \\nu \\,|\\, \\theta, \\lambda) &= U(t) =  - \\frac{N}{2} \\ln 2\\pi + \\frac{1}{2} \\ln |\\tilde\\Pi| -  \\frac{1}{2}{{\\tilde\\varepsilon}^T \\tilde\\Pi \\tilde\\varepsilon} + \\ln p(\\tilde \\nu)\n",
    "\\end{align*}\n",
    "\n",
    "where the first term is constant, and the fourth term is defined by the input causes and considered known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequences ###\n",
    "Our observations are often not in the form of generalised coordinates, but instead as a sequence of observations in time. We can use Taylor's theorem to link the two noting that the generalised coordinates contain the higher order derivatives of the state at time $t$. Friston writes this as:\n",
    "\n",
    "\\begin{align*}\n",
    "y &= [y(1), \\dots, y(N)]^T \\\\\n",
    "y &= \\tilde E(t) \\tilde y(t) \\\\\n",
    "\\tilde E(t) &= E(t) \\otimes I \\\\\n",
    "E(t)_{ij} &= \\frac{(i-t)^{j-1}}{(j-1)!}\\\\\n",
    "\\tilde y(t) &= \\tilde E(t)^{-1} y\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The relation between $i$ and $t$ is not entirely clear to me in his formulation, nor is the numbering of the levels. For a single state we can generate a sequence $y = [y[t - 1\\Delta t], \\dots, y[t - N\\Delta t]]^T$ from the generalised coordinates $\\tilde y$ with elements $\\overset{j}{y}$ where $j = 0 \\dots n$, using the Taylor expansion:\n",
    "\n",
    "\\begin{align*}\n",
    "y &= E(t) \\tilde y(t) \\\\\n",
    "\\begin{bmatrix}\n",
    "y(t-1\\Delta t) \\\\\n",
    "y(t-2\\Delta t) \\\\\n",
    "\\vdots \\\\\n",
    "y(t-N\\Delta t) \\\\\n",
    "\\end{bmatrix}\n",
    " &= \n",
    "\\begin{bmatrix}\\frac{(-1\\Delta t)^0}{0!}& \\frac{(-1\\Delta t)^1}{1!}& \\frac{(-1\\Delta t)^2}{2!}& ...& \\frac{(-1\\Delta t)^n}{n!} \\\\\n",
    "\\frac{(-2\\Delta t)^0}{0!}& \\frac{(-2\\Delta t)^1}{1!}& \\frac{(-2\\Delta t)^2}{2!}& ...& \\frac{(-2\\Delta t)^n}{n!} \\\\\n",
    "& & \\vdots \\\\\n",
    "\\frac{(-N\\Delta t)^0}{0!}& \\frac{(-N\\Delta t)^1}{1!}& \\frac{(-N\\Delta t)^2}{2!}& ...& \\frac{(-N\\Delta t)^n}{n!}\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\overset{0}{y(t)}\\\\\n",
    "\\overset{1}{y(t)}\\\\\n",
    "\\overset{2}{y(t)}\\\\\n",
    "\\vdots \\\\\n",
    "\\overset{n}{y(t)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "E(t)_{ij} &= \\frac{(-(i+1)\\Delta t)^{j}}{(j)!}\\\\\n",
    "\\tilde y(t) &= E(t)^{-1} y\\\\\n",
    "\\end{align*}\n",
    "\n",
    "For this to work, $E(t)^{-1}$ needs to exist, meaning that it needs to be a square matrix and the number of elements in the sequence $N$ ($i = 0, \\dots, N-1$) equals the number of levels in the generalised coordinates $n+1$ (the $j$s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fixed matrix for a fixed number of levels in the generalised coordinates. So let's calculate this up to the sixth derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E = \n",
      "[1 1 1 1 1 1 1] / 1\n",
      "[1 2 3 4 5 6 7] / 1\n",
      "[ 1  4  9 16 25 36 49] / 2\n",
      "[  1   8  27  64 125 216 343] / 6\n",
      "[   1   16   81  256  625 1296 2401] / 24\n",
      "[    1    32   243  1024  3125  7776 16807] / 120\n",
      "[     1     64    729   4096  15625  46656 117649] / 720\n"
     ]
    }
   ],
   "source": [
    "from math import factorial as fac\n",
    "from numpy import zeros\n",
    "from numpy.linalg import inv\n",
    "\n",
    "E = zeros((7, 7))\n",
    "print(\"E = \")\n",
    "for j in range(7):\n",
    "    for i in range(7):\n",
    "        E[i, j] = ((i+1)**(j))/fac(j)\n",
    "    print(\"{:} / {:}\".format((E[:, j] * fac(j)).astype(int), fac(j)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_inv = \n",
      "[[   7.          -21.           35.          -35.           21.\n",
      "    -7.            1.        ]\n",
      " [ -11.15         43.95        -79.08333333   82.          -50.25\n",
      "    16.98333333   -2.45      ]\n",
      " [  14.17777778  -65.48333333  129.66666667 -141.38888889   89.33333333\n",
      "   -30.81666667    4.51111111]\n",
      " [ -13.875        71.         -152.375       176.         -115.625\n",
      "    41.           -6.125     ]\n",
      " [   9.83333333  -54.          123.5        -150.66666667  103.5\n",
      "   -38.            5.83333333]\n",
      " [  -4.5          26.          -62.5          80.          -57.5\n",
      "    22.           -3.5       ]\n",
      " [   1.           -6.           15.          -20.           15.\n",
      "    -6.            1.        ]]\n"
     ]
    }
   ],
   "source": [
    "E_inv = inv(E)\n",
    "print(\"E_inv = \")\n",
    "print(E_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for the third row, it looks like each row can be turned into integers by multiplying with the inverse factorial sequence $(7-i)!$. For the third row, an additional factor 3 is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  35279 -105839  176400 -176400  105840  -35280    5040] / 5040\n",
      "[ -8027  31644 -56940  59040 -36180  12228  -1764] / 720\n",
      "[  5103 -23574  46680 -50900  32160 -11094   1624] / 360\n",
      "[ -333  1704 -3657  4224 -2775   984  -147] / 24\n",
      "[  59 -324  741 -904  621 -228   35] / 6\n",
      "[  -9   52 -125  160 -115   44   -7] / 2\n",
      "[  1  -6  15 -20  15  -6   1] / 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(\"{:} / {:}\".format((E_inv[i] * fac(7-i) * (1 + 2*(i==2))).astype(int), fac(7-i)* (1 + 2*(i==2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, the sum of each row should be zero except for the first row, so that if the sequence $y$ is constant, the zeroth order generalised coordinate is equal to that constant, and all the derivatives are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00 -2.46469511e-13  4.44089210e-15 -3.55271368e-14\n",
      "  5.68434189e-14 -1.90958360e-14  4.88498131e-15]\n",
      "[1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from numpy import sum\n",
    "print(sum(E_inv, axis=1))\n",
    "print(sum(E_inv, axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Expectation Maximisation ###\n",
    "Above, we showed that under the Laplace approximation, the optimal precision of each partition can be found by calculating the second derivative (Hessian) of the internal entergy and evaluating it at the mode of that partition. To find the modes we use an optimisation technique. As with conventional variational schemes, we can update the modes of our three partitions in three distinct steps. However, the step dealing with the state (**D**-step) must integrate its conditional mode $\\tilde \\mu := \\mu^u(t)$ over time to accumulate the quantities necessary for updating the parameters (**E**-step) and hyperparameters (**M**-step). We now consider optimising the modes or conditional means in each of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The D-step ####\n",
    "\n",
    "$\\require{color}$In static systems, the mode of the conditional density maximises variational energy, such that $V(t)_u = 0$; this is the solution to a gradient ascent scheme: $\\dot{\\tilde \\mu} = V(t)_u$. In dynamic systems, we also require the path of the mode to be the mode of the path: $\\dot{\\tilde \\mu} = D \\tilde \\mu$. These two conditions can be satisfied by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{\\tilde \\mu} - D \\tilde \\mu = V(t)_u \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\dot{\\tilde \\mu} - D \\tilde \\mu $ can be regarded as motion in a frame of reference that moves along the trajectory encoded by the generalised coordinates. The stationary solution in this moving frame of reference maximises variational action. \n",
    "\n",
    "So far, all our discussions have assumed we're operating in continuous time. However, we would execute the D-step at discrete intervals. To apply this we linearise the trajectory following (Ozaki, 1992):\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta \\tilde \\mu &= J(t)^{-1} \\left( \\exp(J(t) \\Delta t) - I \\right) \\dot{\\tilde \\mu} \\\\\n",
    "J(t) &:= \\frac{\\partial \\dot{\\tilde \\mu}}{\\partial u} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note, this linearisation simply reduces to the Taylor series expansion around $\\tilde \\mu(t)$ to get $\\tilde \\mu(t+\\Delta t)$ for a linear system where $\\dot {\\tilde \\mu} = J(t) \\tilde \\mu$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\exp(x) &= 1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots \\\\\n",
    "\\exp(J(t) \\Delta t) - 1 &= J(t) \\Delta t + \\frac{J(t)^2 \\Delta t^2}{2!} + \\frac{J(t)^3 \\Delta t^3}{3!} + \\cdots \\\\\n",
    "J(t)^{-1} \\left(\\exp(J(t) \\Delta t) - 1 \\right) &= \\Delta t + \\frac{J(t) \\Delta t^2}{2!} + \\frac{J(t)^2 \\Delta t^3}{3!} + \\cdots \\\\\n",
    "\\tilde \\mu(t) + J(t)^{-1} \\left(\\exp(J(t) \\Delta t) - 1 \\right) \\dot{\\tilde \\mu} &= \\tilde \n",
    "\\mu(t) + (\\Delta t + \\frac{J(t) \\Delta t^2}{2!} + \\frac{J(t)^2 \\Delta t^3}{3!} + \\cdots) \\dot{\\tilde \\mu} \\\\\n",
    "&= \\tilde \n",
    "\\mu(t) + (\\Delta t + \\frac{J(t) \\Delta t^2}{2!} + \\frac{J(t)^2 \\Delta t^3}{3!} + \\cdots) J(t) \\tilde \\mu \\\\\n",
    "&= \\tilde \n",
    "\\mu(t) + \\frac{J(t) \\Delta t}{1!} + \\frac{(J(t) \\Delta t)^2}{2!} + \\frac{(J(t) \\Delta t)^3}{3!} + \\cdots \\\\\n",
    "&= \\tilde \\mu(t + \\Delta t) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the D step becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\dot {\\tilde y} \\\\\n",
    "\\dot {\\tilde \\mu} \\\\\n",
    "\\dot {\\tilde \\eta} \\\\\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "D \\tilde y \\\\\n",
    "V(t)_u + D \\tilde \\mu \\\\\n",
    "D \\tilde \\eta \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "J(t) &= \n",
    "\\begin{bmatrix}\n",
    "D & 0& 0 \\\\\n",
    "V(t)_{uy} & V(t)_{uu} + D & V(t)_{u\\eta}\\\\\n",
    "0 & 0 & D \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "U(t) &= - \\frac{N}{2} \\ln 2\\pi + \\frac{1}{2} \\ln |\\tilde\\Pi| -  \\frac{1}{2}{{\\tilde\\varepsilon}^T \\tilde\\Pi \\tilde\\varepsilon} + \\ln p(\\tilde \\nu) \\\\\n",
    "V(t) &= U(t) + W(t)^\\theta + W(t)^\\lambda  - \\left< \\ln Z^\\theta \\right>_{q(\\theta)} - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)}\\\\\n",
    "V(t)_u &= U(t)_u + W(t)_u^\\theta \\\\\n",
    "V(t)_{uu} &= U(t)_{uu} + W(t)_{uu}^\\theta \\\\\n",
    "V(t)_{uy} &= U(t)_{uy}\\\\\n",
    "V(t)_{u\\eta} &= U(t)_{u\\eta} \\\\\n",
    "U(t)_u &= - \\tilde \\varepsilon_u^T \\tilde \\Pi \\tilde \\varepsilon \\\\\n",
    "U(t)_{uu} &= - \\tilde \\varepsilon_u^T \\tilde \\Pi \\tilde \\varepsilon_u \\\\\n",
    "U(t)_{uy} &= -\\tilde \\varepsilon_u^T \\tilde \\Pi \\tilde \\varepsilon_y \\\\\n",
    "U(t)_{u\\eta} &= -\\tilde \\varepsilon_u^T \\tilde \\Pi \\tilde \\varepsilon_\\eta \\\\\n",
    "U(t)_{\\theta \\theta} &= - \\tilde \\varepsilon_\\theta^T \\tilde \\Pi \\tilde \\varepsilon_\\theta \\\\\n",
    "U(t)_{\\lambda \\lambda} &= - \\tilde \\varepsilon_\\lambda^T \\tilde \\Pi \\tilde \\varepsilon_\\lambda \\\\\n",
    "W(t)^u &= \\frac{1}{2} \\text{tr}(\\Sigma^u U(t)_{uu}) = -\\frac{1}{2} \\text{tr}(\\Sigma^u \\tilde \\varepsilon_u^T \\tilde \\Pi \\tilde \\varepsilon_u) \\\\\n",
    "W(t)^\\theta &= \\frac{1}{2} \\text{tr}(\\Sigma^\\theta U(t)_{\\theta\\theta}) = -\\frac{1}{2} \\text{tr}(\\Sigma^\\theta \\tilde \\varepsilon_\\theta^T \\tilde \\Pi \\tilde \\varepsilon_\\theta) \\\\\n",
    "W(t)^\\lambda &= \\frac{1}{2} \\text{tr}(\\Sigma^\\lambda U(t)_{\\lambda\\lambda}) = -\\frac{1}{2} \\text{tr}(\\Sigma^\\lambda \\tilde \\varepsilon_\\lambda^T \\tilde \\Pi \\tilde \\varepsilon_\\lambda) \\\\\n",
    "W(t)_{u_i}^\\theta &= -\\frac{1}{2} \\text{tr} \\left( \\Sigma^\\theta \\tilde \\varepsilon_{\\theta u_i}^T \\tilde \\Pi \\tilde \\varepsilon_\\theta \\right) \\\\\n",
    "W(t)_{uu_{ij}}^\\theta &= -\\frac{1}{2} \\text{tr} \\left( \\Sigma^\\theta \\tilde \\varepsilon_{\\theta u_i}^T \\tilde \\Pi \\tilde \\varepsilon_{\\theta u_j} \\right) \\\\\n",
    "\\tilde\\varepsilon &= \n",
    "    \\begin{bmatrix}\n",
    "    \\tilde\\varepsilon^\\nu = \\tilde y - \\tilde g   \\\\\n",
    "    \\tilde\\varepsilon^x = D\\tilde x - \\tilde f\n",
    "    \\end{bmatrix} \\\\\n",
    "\\tilde\\varepsilon_u &= \n",
    "    \\begin{bmatrix}\n",
    "    \\tilde\\varepsilon_\\nu^\\nu & \\tilde\\varepsilon_x^\\nu \\\\\n",
    "    \\tilde\\varepsilon_\\nu^x & \\tilde\\varepsilon_x^x \\\\\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    g_\\nu & g_x \\\\\n",
    "    f_\\nu & f_x \\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "\\tilde \\varepsilon_y &=\n",
    "\\begin{bmatrix}\n",
    "\\tilde \\varepsilon_y^\\nu = I \\\\\n",
    "\\tilde \\varepsilon_y^x = 0 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\tilde \\varepsilon_\\eta &=\n",
    "\\begin{bmatrix}\n",
    "\\tilde \\varepsilon_\\eta^\\nu = I \\\\\n",
    "\\tilde \\varepsilon_\\eta^x = 0 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\tilde\\varepsilon_{u \\theta} &= \\tilde\\varepsilon_{\\theta u}^T =\n",
    "    \\begin{bmatrix}\n",
    "    g_{\\nu \\theta} & g_{x \\theta} \\\\\n",
    "    f_{\\nu \\theta} & f_{x \\theta} \\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "\\tilde\\varepsilon_\\theta &= \\tilde\\varepsilon_{u \\theta} \\mu^u \\\\\n",
    "\\tilde\\varepsilon_{u \\lambda} &= \n",
    "    \\begin{bmatrix}\n",
    "    g_{\\nu \\lambda} & g_{x \\lambda} \\\\\n",
    "    f_{\\nu \\lambda} & f_{x \\lambda} \\\\\n",
    "    \\end{bmatrix}  = \\mathbf{0} \\\\\n",
    "\\tilde\\varepsilon_\\lambda &= \\tilde\\varepsilon_{u \\lambda} \\mu^u = \\mathbf{0}\\\\\n",
    "\\Delta \\tilde \\mu &= \\left( \\exp(\\Delta t J(t)) - I \\right) J(t)^{-1} (V(t)_{u} + D \\tilde \\mu) \\\\\n",
    "\\Pi^u &= -U(t)_{uu}|_\\mu = \\{\\tilde \\varepsilon_u^T \\tilde \\Pi \\tilde \\varepsilon_u\\}|_\\mu \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The mean-field term, $W(t)^\\lambda$ does not contribute to the D-step because it is not a function of the states, since the hyperparameters only control the amplitude and smoothness of the random fluctuations and $g$ and $f$ do not depend on the hyperparameters. This means uncertainly about the hyperparameters does not affect the update for the states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The E- and M-steps ####\n",
    "\n",
    "Exactly the same update procedure can be used for the **E**-step (parameter update) and **M**-step (hyperparameter update). However, in this instance there are no generalised coordinates to consider as we consider them stationary over the period of inference. Furthermore, we can set the interval between updates to be arbitrarily long because the parameters are updated after the time-series has been integrated. If $\\Delta t \\to \\infty$ is sufficiently large, the matrix exponential in the Ozaki linearisation disappears (because the curvature of the Jacobian is negative definite) giving:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta \\mu^\\theta &= -J(\\theta)^{-1} \\dot\\mu^\\theta \\\\\n",
    "\\dot\\mu^\\theta & = \\bar V_{\\theta}^\\theta \\\\\n",
    "J(\\theta) &= \\bar V_{\\theta \\theta}^\\theta \\\\\n",
    "\\Delta \\mu^\\lambda &= -J(\\lambda)^{-1} \\dot\\mu^\\lambda \\\\\n",
    "\\dot\\mu^\\lambda & = \\bar V_{\\lambda}^\\lambda \\\\\n",
    "J(\\lambda) &= \\bar V_{\\lambda \\lambda}^\\lambda \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This  is a conventional Gauss-Newton update scheme. In this sense, the D-Step can be regarded as a generalization of classical ascent schemes to generalised coordinates that cover dynamic systems. For our model, the requisite gradients and curvatures of variational action for the E-step are:\n",
    "\n",
    "\\begin{align*}\n",
    "U(t) &= - \\frac{N}{2} \\ln 2\\pi + \\frac{1}{2} \\ln |\\tilde\\Pi| -  \\frac{1}{2}{{\\tilde\\varepsilon}^T \\tilde\\Pi \\tilde\\varepsilon} + \\ln p(\\tilde \\nu) \\\\\n",
    "\\bar{V}^\\theta &= \\int U(t) + W(t)^u + W(t)^\\lambda - \\left< \\ln Z^u \\right>_{q(u)} - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)} \\:dt + U^\\theta \\\\\n",
    "&= \\int U(t) + W(t)^u + W(t)^\\lambda - \\left< \\ln Z^u \\right>_{q(u)} - \\left< \\ln Z^\\lambda \\right>_{q(\\lambda)} \\:dt + {\\varepsilon^\\theta}^T \\Pi^\\theta \\varepsilon^\\theta - \\ln Z^\\theta \\\\\n",
    "\\bar V_\\theta^\\theta &= \\int \\tilde \\varepsilon_\\theta^T \\tilde \\Pi \\tilde \\varepsilon + W(t)_\\theta^u \\:dt - \\Pi^\\theta \\varepsilon^\\theta \\\\\n",
    "\\bar V_{\\theta \\theta}^\\theta &= \\int \\tilde \\varepsilon_\\theta^T \\tilde \\Pi \\tilde \\varepsilon_\\theta + W(t)_{\\theta \\theta}^u \\:dt - \\Pi^\\theta \\\\\n",
    "W(t)_{\\theta_i}^u &= -\\frac{1}{2} \\text{tr} \\left( \\Sigma_t^u \\tilde \\varepsilon_{u \\theta_i}^T \\tilde \\Pi \\tilde \\varepsilon_u \\right) \\\\\n",
    "W(t)_{\\theta_i \\theta_j}^u &= -\\frac{1}{2} \\text{tr} \\left( \\Sigma_t^u \\tilde \\varepsilon_{u \\theta_i}^T \\tilde \\Pi \\tilde \\varepsilon_{u \\theta_j} \\right) \\\\\n",
    "J(\\theta) &= \\bar V_{\\theta \\theta}^\\theta \\\\\n",
    "\\Delta \\mu^\\theta &= -J(\\theta)^{-1} \\dot\\mu^\\theta = -{\\bar V_{\\theta \\theta}^\\theta}^{-1} \\bar V_{\\theta}^\\theta \\\\\n",
    "\\Pi^\\theta &= -\\bar{U}_{\\theta\\theta}|_{\\mu^\\theta} = - \\int U(t)_{\\theta\\theta}|_{\\mu^\\theta} \\: dt - U^\\theta_{\\theta\\theta} \\\\\n",
    "U(t)_{\\theta \\theta} &= - \\tilde \\varepsilon_\\theta^T \\tilde \\Pi \\tilde \\varepsilon_\\theta \\\\\n",
    "U^\\theta_{\\theta\\theta} &= -{C^\\theta}^{-1} \\\\\n",
    "\\Pi^\\theta &= {C^\\theta}^{-1} + \\int \\tilde \\varepsilon_\\theta^T \\tilde \\Pi \\tilde \\varepsilon_\\theta \\: dt \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $Z^\\theta$ is a normalisation constant. Here the precision matrix is updated from the prior precision ${C^\\theta}^{-1}$. If we assume the parameters are not constant, but varying much more slowly than the states, then this could be estimated iteratively, using the last estimate of the precision as the prior for the next one, assuming sufficient observations per update. In this case we would use:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta \\Pi^\\theta &= \\int \\tilde \\varepsilon_\\theta^T \\tilde \\Pi \\tilde \\varepsilon_\\theta \\:dt \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, for the hyperparameters, but in this case the precision matrix $\\tilde\\Pi$ is a function of the hyperparameters $\\lambda$:\n",
    "\n",
    "\\begin{align*}\n",
    "U(t) &= - \\frac{N}{2} \\ln 2\\pi + \\frac{1}{2} \\ln |\\tilde\\Pi| -  \\frac{1}{2}{{\\tilde\\varepsilon}^T \\tilde\\Pi \\tilde\\varepsilon} + \\ln p(\\tilde \\nu) \\\\\n",
    "\\bar{V}^\\lambda &= \\int U(t) + W(t)^u + W(t)^\\theta  - \\left< \\ln Z^u \\right>_{q(u)} - \\left< \\ln Z^\\theta \\right>_{q(\\theta)} \\:dt + U^\\lambda \\\\\n",
    "&= \\int U(t) + W(t)^u + W(t)^\\theta - \\left< \\ln Z^u \\right>_{q(u)} - \\left< \\ln Z^\\theta \\right>_{q(\\theta)} \\:dt + {\\varepsilon^\\lambda}^T \\Pi^\\lambda \\varepsilon^\\lambda - \\ln Z^\\lambda \\\\\n",
    "\\bar V_\\lambda^\\lambda &= \\int \\tilde \\varepsilon_\\lambda^T \\tilde \\Pi \\tilde \\varepsilon + W(t)_\\lambda^u + W(t)_\\lambda^\\theta \\:dt - \\Pi^\\lambda \\varepsilon^\\lambda \\\\\n",
    "\\bar V_{\\lambda \\lambda}^\\lambda &= \\int \\tilde \\varepsilon_\\lambda^T \\tilde \\Pi \\tilde \\varepsilon_\\lambda + W(t)_{\\lambda \\lambda}^u \\:dt - \\Pi^\\lambda \\\\\n",
    "W(t)_{\\lambda_i}^u &= -\\frac{1}{2} \\text{tr} \\left( \\Sigma_i^u \\tilde \\varepsilon_{u}^{iT} \\tilde\\Pi_{\\lambda_i} \\tilde \\varepsilon_u^i \\right) \\\\\n",
    "W(t)_{\\lambda_i}^\\theta &= -\\frac{1}{2} \\text{tr} \\left( \\Sigma_i^\\theta \\tilde \\varepsilon_{\\theta}^{iT} \\tilde\\Pi_{\\lambda_i} \\tilde \\varepsilon_\\theta^i \\right) \\\\\n",
    "W(t)_{\\lambda \\lambda}^u &= 0 \\\\\n",
    "J(\\lambda) &= \\bar V_{\\lambda \\lambda}^\\lambda \\\\\n",
    "\\Delta \\mu^\\lambda &= -J(\\lambda)^{-1} \\dot\\mu^\\lambda = - {\\bar V_{\\lambda \\lambda}^\\lambda}^{-1} \\bar V_{\\lambda}^\\lambda\\\\\n",
    "\\Pi^\\lambda &= -\\bar{U}_{\\lambda\\lambda} = - \\int U(t)_{\\lambda\\lambda} \\: dt- U^\\lambda_{\\lambda\\lambda} \\\\\n",
    "U(t)_{\\lambda \\lambda} &= - \\tilde \\varepsilon_\\lambda^T \\tilde \\Pi \\tilde \\varepsilon_\\lambda \\\\\n",
    "U^\\lambda_{\\lambda\\lambda} &= -{C^\\lambda}^{-1} \\\\\n",
    "\\Pi^\\lambda &= {C^\\lambda}^{-1} + \\int \\tilde \\varepsilon_\\lambda^T \\tilde \\Pi \\tilde \\varepsilon_\\lambda \\:dt \\\\\n",
    "&\\text{or}\\\\\n",
    "\\Delta \\Pi^\\lambda &= \\int \\tilde \\varepsilon_\\lambda^T \\tilde \\Pi \\tilde \\varepsilon_\\lambda \\:dt \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $W(t)_{\\lambda \\lambda}^u = 0$ by definition because we assume that the system is linear in the hyperparameters. Although uncertainty about the hyperparameters does not affect the states and parameters, uncertainty about both the states and parameters affect the hyperparameter update. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical dynamic model ###\n",
    "\n",
    "For a hierarchical dynamic model (HDM), we assume each higher level generates causes for the level below, so that the causes $\\nu$ link levels, whereas hidden states $x$ link dynamics over time. Further it is assumed that the noise processes at each level $w^{(i)}$ and $z^{(i)}$ are conditionally independent. This leads to the following Bayesian directed graph:\n",
    "\n",
    "<img src=\"HDM.png\" width=\"600\">\n",
    "\n",
    "Here $\\vartheta^{(i)} = [\\theta^{(i)}, \\lambda^{(i)}]$ and $u^{(i)} = [\\tilde \\nu^{(i)}, \\tilde x^{(i)}]$ and:\n",
    "\n",
    "\\begin{align*}\n",
    "g_\\nu &= \n",
    "\\begin{bmatrix}\n",
    "g_\\nu^{(1)} & & \\\\\n",
    "0 & \\ddots & \\\\\n",
    "& \\ddots & g_\\nu^{(m)} \\\\\n",
    "& & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "&g_x = \n",
    "\\begin{bmatrix}\n",
    "g_x^{(1)} & & \\\\\n",
    "0 & \\ddots & \\\\\n",
    "& \\ddots & g_x^{(m)} \\\\\n",
    "& & 0 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "f_\\nu &= \n",
    "\\begin{bmatrix}\n",
    "f_\\nu^{(1)} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & f_\\nu^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "&f_x = \n",
    "\\begin{bmatrix}\n",
    "f_x^{(1)} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & f_x^{(m)} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that the partial derivatives of $g(x,\\nu)$ have an extra row to accommodate the highest hierarchical level."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
